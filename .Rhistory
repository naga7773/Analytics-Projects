lda_acc <- mean(lda.class == od$rating)
lda_acc <- mean(lda.class == od$rating)
lda_acc <- mean(lda.class == od$rating)
lda_acc <- mean(lda.class == od$rating)
### Multinomial Logistic regression
require(nnet)
mlr.fit <- multinom(data = od, rating~.-quality-quality_factor, family = "binomial")
mlr.pred <- predict(lr.fit, type = "class")
table(mlr.pred, od$rating)
mlr_acc <- mean(mlr.pred == od$rating)
mlr_acc
### QDA ###
qda.fit <- qda(data =od,rating~.-quality-quality_factor)
qda.class <- predict(lda.fit, od)$class
table(qda.class, od$rating)
qda_acc <- mean(qda.class == od$rating)
qda_acc
lda_acc
mlr_acc
rf_acc
### SVC ###
require(e1071)
install.packages("e1071")
### SVC ###
library(e1071)
svm.fit <- svm(data = od, rating~.-quality-quality_factor, kernel = "radial", cost = 10, gamma = 1)
svm.pred <- predict(svm.fit, od)
table(svm.pred, od$rating)
svm_acc <- mean(svm.pred == od$rating)
svm_acc
tune.out <- tune(svm,rating~.-quality-quality_factor, data = od, kernel = "linear")
summary(tune.out$best.model)
svm_linear_tune<- tune(svm,rating~.-quality-quality_factor, data = od, kernel = "linear",
range=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
svm.pred <- predict(svm.fit, od)
svm.pred <- predict(svm.fit, od)
svm.pred <- predict(svm.fit, od)
svm.pred <- predict(svm.fit, od)
table(svm.pred, od$rating)
svm_acc <- mean(svm.pred == od$rating)
dim(od)[1]
#Split data in to train and test
split_ratio <-  0.7
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
#svm_linear_model <- svm_linear_tune$best.model
#svm_radial_model <- svm_radial_tune$best.model
svm.fit <- svm(data = train_data, rating~.-quality-quality_factor, kernel = "radial", cost = 10, gamma = 1)
svm.pred <- predict(svm.fit, od)
svm.pred <- predict(svm.fit, newdata = test_data)
table(svm.pred, od$rating)
table(svm.pred, test_data$rating)
svm_acc <- mean(svm.pred == od$rating)
svm_acc <- mean(svm.pred == test_data$rating)
svm_acc
rf.pred <- predict(rf.fit, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
rf_acc
#Split data in to train and test
split_ratio <-  0.75
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
#### RandomForest ######
rf.fit <- randomForest(rating~.-quality-quality_factor,data = train_data, mtry = 3, importance = T)
rf.pred <- predict(rf.fit, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
rf_acc
#Split data in to train and test
split_ratio <-  0.7
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
#### RandomForest ######
rf.fit <- randomForest(rating~.-quality-quality_factor,data = train_data, mtry = 3, importance = T)
rf.pred <- predict(rf.fit, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
rf_acc
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
#### RandomForest ######
rf.fit <- randomForest(rating~.-quality-quality_factor,data = train_data, mtry = 3, importance = T)
rf.pred <- predict(rf.fit, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
rf_acc
#Split data in to train and test
split_ratio <-  0.9
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
#### RandomForest ######
rf.fit <- randomForest(rating~.-quality-quality_factor,data = train_data, mtry = 3, importance = T)
rf.pred <- predict(rf.fit, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
rf_acc
#Split data in to train and test
split_ratio <-  0.7
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
#### RandomForest ######
rf.fit <- randomForest(rating~.-quality-quality_factor,data = train_data, mtry = 3, importance = T)
rf.pred <- predict(rf.fit, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
rf_acc
### LDA #####
lda.fit <- lda(data = train_data, rating~.-quality-quality_factor)
lda.class <- predict(lda.fit, newdata = test_data)$class
table(lda.class, test_data$rating)
lda_acc <- mean(lda.class == test_data$rating)
lda_acc
qda.class <- predict(lda.fit, newdata = test_data)$class
### QDA ###
qda.fit <- qda(data = train_data, rating~.-quality-quality_factor)
qda.class <- predict(lda.fit, newdata = test_data)$class
table(qda.class, test_data$rating)
qda_acc <- mean(qda.class == test_data$rating)
qda_acc
### QDA ###
qda.fit <- qda(data = train_data, rating~.-quality-quality_factor)
qda.class <- predict(qda.fit, newdata = test_data)$class
table(qda.class, test_data$rating)
qda_acc <- mean(qda.class == test_data$rating)
qda_acc
mlr.fit <- multinom(data = train_data, rating~.-quality-quality_factor, family = "binomial")
mlr.pred <- predict(lr.fit, newdata = test_data, type = "class") #type="probs" gives prob of every class per obs
table(mlr.pred, od$rating)
table(mlr.pred, test_data$rating)
mlr_acc <- mean(mlr.pred == test_data$rating)
mlr_acc
svm_acc
#svm_linear_model <- svm_linear_tune$best.model
#svm_radial_model <- svm_radial_tune$best.model
svm.fit <- svm(data = train_data, rating~.-quality-quality_factor, kernel = "radial", cost = 10, gamma = 1)
svm.pred <- predict(svm.fit, newdata = test_data)
table(svm.pred, test_data$rating)
svm_acc <- mean(svm.pred == test_data$rating)
svm_acc
mlr_acc
lda_acc
qda_acc
rf_acc
svm_acc
a<- class.ind(train_data$rating)
View(a)
response_matrix <- class.ind(train_data$rating)
nn.fit <- nnet(train_data[,c(1:11)], response_matrix, size =10, softmax = T)
nn.fit <- nnet(train_data[,c(1:11)], response_matrix, size =10)
nn.fit <- nnet(train_data[,c(1:11)], response_matrix, size =10, softmax = T)
nn.pred <- predict(nn.fit, test_data[,c(1:11)] , type="class")
table(nn.pred, test_data$rating)
nn_acc <- mean(nn.pred == test_data$rating)
nn_acc
od$rating <- factor(od$rating, levels = c("bad","average","good"))
dim(train_data)
train_data %>%
summarise(avg = mean(fixed.acidity),
dev = sd(fixed.acidity),
obs = n())
train_data %>%
group_by(rating) %>%
summarise(avg = mean(fixed.acidity),
dev = sd(fixed.acidity),
obs = n())
t <- function(x){
a <- train_data %>%
group_by(rating) %>%
summarise(avg = mean(fixed.acidity),
dev = sd(fixed.acidity),
obs = n())
return(a)
}
t(fixed.acidity)
t(density)
lapply(train_data, t)
t <- function(x){
a <- train_data %>%
group_by(rating) %>%
summarise(avg = mean(x),
dev = sd(x),
obs = n())
return(a)
}
lapply(train_data, t)
lapply(train_data[,c(1:11)], t)
train_data %>%
select(volatile.acidity)%>%
ggplot()+
geom_density(aes(x = volatile.acidity))
e <- train_data %>%
select(volatile.acidity)
train_data %>%
dplyr::select(volatile.acidity)%>%
ggplot()+
geom_density(aes(x = volatile.acidity))
train_data %>%
dplyr::select(volatile.acidity,rating)%>%
ggplot()+
geom_density(aes(x = volatile.acidity, fill = rating))
train_data %>%
dplyr::select(volatile.acidity,rating)%>%
ggplot()+
geom_boxplot(aes(x = volatile.acidity, fill = rating))
train_data %>%
dplyr::select(volatile.acidity,rating)%>%
ggplot()+
geom_boxplot(aes(y = volatile.acidity, x = rating, fill = rating))
lapply(train_data[,c(1:11)], t)
t(volatile.acidity)
t <- function(x){
a <- train_data %>%
group_by(rating) %>%
summarise(avg = mean(x),
dev = sd(x),
obs = n())
return(a)
}
t(volatile.acidity)
t(alcohol)
t(fixed.acidity)
t <- function(x){
a <- train_data %>%
group_by(rating) %>%
summarise(avg = mean(fixed.acidity),
dev = sd(fixed.acidity),
obs = n())
return(a)
}
t(fixed.acidity)
train_data %>%
dplyr::select(volatile.acidity,rating)%>%
ggplot()+
geom_boxplot(aes(y = volatile.acidity, x = rating, fill = rating))
train_data %>%
dplyr::select(volatile.acidity,rating)%>%
ggplot()+
geom_density(aes(x = volatile.acidity, fill = rating))
train_data %>%
dplyr::select(volatile.acidity,rating)%>%
ggplot()+
geom_density(aes(x = volatile.acidity, fill = rating, alpha = 0.3))
train_data %>%
dplyr::select(fixed.acidity,rating)%>%
ggplot()+
geom_density(aes(x = fixed.acidity, fill = rating, alpha = 0.3))
train_data %>%
dplyr::select(fixed.acidity,rating)%>%
ggplot()+
geom_density(aes(x = fixed.acidity, fill = rating, alpha = 0.3))
train_data %>%
dplyr::select(density,rating)%>%
ggplot()+
geom_density(aes(x = density, fill = rating, alpha = 0.3))
rm(list=ls())
setwd("/Users/naga/Downloads/My files/My code files/R/")
OriginalData <- read.csv("winequality-white.csv", sep = ';')
od <- OriginalData[,]
od$quality_factor <- as.factor(od$quality)
od <- od %>%
mutate(rating = ifelse(quality<=5,"bad",ifelse(quality<=7,"average","good")))
od$rating <- factor(od$rating, levels = c("bad","average","good"))
split_ratio <-  0.7
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
train_data %>%
group_by(rating) %>%
summarise(avg = mean(fixed.acidity),
dev = sd(fixed.acidity),
obs = n())
#### RandomForest ######
rf.fit <- randomForest(rating~.-quality-quality_factor,data = train_data, mtry = 3, importance = T)
rf.pred <- predict(rf.fit, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
rf_acc
### LDA #####
lda.fit <- lda(data = train_data, rating~.-quality-quality_factor)
lda.class <- predict(lda.fit, newdata = test_data)$class
table(lda.class, test_data$rating)
lda_acc <- mean(lda.class == test_data$rating)
lda_acc
### QDA ###
qda.fit <- qda(data = train_data, rating~.-quality-quality_factor)
qda.class <- predict(qda.fit, newdata = test_data)$class
table(qda.class, test_data$rating)
qda_acc <- mean(qda.class == test_data$rating)
qda_acc
### Multinomial Logistic regression ###
require(nnet)
mlr.fit <- multinom(data = train_data, rating~.-quality-quality_factor, family = "binomial")
mlr.pred <- predict(lr.fit, newdata = test_data, type = "class") #type="probs" gives prob of every class per obs
mlr.pred <- predict(mlr.fit, newdata = test_data, type = "class") #type="probs" gives prob of every class per obs
table(mlr.pred, test_data$rating)
mlr_acc <- mean(mlr.pred == test_data$rating)
mlr_acc
### SVC ###
library(e1071)
#svm_linear_model <- svm_linear_tune$best.model
#svm_radial_model <- svm_radial_tune$best.model
svm.fit <- svm(data = train_data, rating~.-quality-quality_factor, kernel = "radial", cost = 10, gamma = 1)
svm.pred <- predict(svm.fit, newdata = test_data)
table(svm.pred, test_data$rating)
svm_acc <- mean(svm.pred == test_data$rating)
svm_acc
response_matrix <- class.ind(train_data$rating)
nn.fit <- nnet(train_data[,c(1:11)], response_matrix, size =10, softmax = T)
nn.pred <- predict(nn.fit, test_data[,c(1:11)] , type="class")
table(nn.pred, test_data$rating)
nn_acc <- mean(nn.pred == test_data$rating)
nn_acc
rf_acc
importance(rf.fit)
train_data %>%
select(rating, density,chlorides, alcohol, fixed.acidity, volatile.acidity)%>%
ggplot()+
geom_point(aes(x = chlorides, y = alcohol, col = rating))
train_data %>%
select(rating,chlorides, alcohol)%>%
ggplot()+
geom_point(aes(x = chlorides, y = alcohol, col = rating))
train_data %>%
dplyr::select(rating,chlorides, alcohol)%>%
ggplot()+
geom_point(aes(x = chlorides, y = alcohol, col = rating))
train_data %>%
dplyr::select(rating,chlorides, alcohol)%>%
ggplot()+
geom_point(aes(y = chlorides, x = alcohol, col = rating))
for (i in c(1:500)){
print(i)
}
test_data <- od[-train_index, ]
for (i in c(1:500)){
split_ratio <-  0.7
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
rf.fit <- randomForest(rating~.-quality-quality_factor,data = train_data, mtry = 3, importance = T)
rf.pred <- predict(rf.fit, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
if(rf_acc > previous_rf_acc){
previous_rf_acc = rf_acc
bestfit = rf.fit
}
}
previous_rf_acc = 0
for (i in c(1:500)){
split_ratio <-  0.7
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
rf.fit <- randomForest(rating~.-quality-quality_factor,data = train_data, mtry = 3, importance = T)
rf.pred <- predict(rf.fit, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
if(rf_acc > previous_rf_acc){
previous_rf_acc = rf_acc
bestfit = rf.fit
}
}
previous_rf_acc = 0
for (i in c(1:20)){
split_ratio <-  0.7
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
rf.fit <- randomForest(rating~.-quality-quality_factor,data = train_data, mtry = 3, importance = T)
rf.pred <- predict(rf.fit, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
if(rf_acc > previous_rf_acc){
previous_rf_acc = rf_acc
bestfit = rf.fit
}
}
previous_rf_acc = 0
for (i in c(1:2)){
split_ratio <-  0.7
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
rf.fit <- randomForest(rating~.-quality-quality_factor,data = train_data, mtry = 3, importance = T)
rf.pred <- predict(rf.fit, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
if(rf_acc > previous_rf_acc){
previous_rf_acc = rf_acc
bestfit = rf.fit
}
}
previous_rf_acc = 0
for (i in c(1:10)){
split_ratio <-  0.7
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
rf.fit <- randomForest(rating~.-quality-quality_factor,data = train_data, mtry = 3, importance = T)
rf.pred <- predict(rf.fit, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
if(rf_acc > previous_rf_acc){
previous_rf_acc = rf_acc
bestfit = rf.fit
}
}
rf.fit.1 <- bestfit
previous_rf_acc = 0
for (i in c(1:20)){
split_ratio <-  0.7
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
rf.fit <- randomForest(rating~.-quality-quality_factor,data = train_data, mtry = 3, importance = T)
rf.pred <- predict(rf.fit, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
if(rf_acc > previous_rf_acc){
previous_rf_acc = rf_acc
bestfit = rf.fit
}
}
#Split data in to train and test
split_ratio <-  0.7
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
#best fit is saved as rf.fit.1
# rf.fit.1
rf.pred <- predict(rf.fit.1, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
rf_acc
#Split data in to train and test
split_ratio <-  0.7
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
#best fit is saved as rf.fit.1
# rf.fit.1
rf.pred <- predict(rf.fit.1, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
rf_acc
#Split data in to train and test
split_ratio <-  0.7
n_obs <- dim(od)[1]
train_index <- sample(c(1:n_obs), floor(split_ratio * n_obs), replace = F)
train_data <- od[train_index, ]
test_data <- od[-train_index, ]
#best fit is saved as rf.fit.1
# rf.fit.1
rf.pred <- predict(rf.fit.1, newdata = test_data)
table(rf.pred, test_data$rating)
rf_acc <- mean(rf.pred == test_data$rating)
rf_acc
getwd()
library(ROCR)
install.packages("ROCR")
library(ROCR)
plot(rf.pred,col=2,lwd=2,main="ROC Curve for Classifiers on Adult Dataset")
plot(rf.fit,col=2,lwd=2,main="ROC Curve for Classifiers on Adult Dataset")
rf.perf <- performance(rf.fit.1,"tpr","fpr")
rf.perf <- performance(rf.pred,"tpr","fpr")
rf.pred
rf.perf <- performance(rf.pred,"tpr","fpr")
#svm_linear_model <- svm_linear_tune$best.model
#svm_radial_model <- svm_radial_tune$best.model
svm.fit <- svm(data = train_data, rating~.-quality-quality_factor, kernel = "radial", cost = 10, gamma = 1)
svm.pred <- predict(svm.fit, newdata = test_data)
table(svm.pred, test_data$rating)
svm_acc <- mean(svm.pred == test_data$rating)
svm_acc
perf <- performance(svm.pred, measure = "tpr", x.measure = "fpr")
pred <- prediction(svm.pred,test_data$rating) )
pred <- prediction(svm.pred,test_data$rating)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
svm.pred[1:5]
svm.pred <- predict(svm.fit, newdata = test_data, probability = T)
svm.fit <- svm(data = train_data, rating~.-quality-quality_factor, kernel = "radial", cost = 10, gamma = 1)
svm.pred <- predict(svm.fit, newdata = test_data)
pred <- prediction(svm.pred,test_data$rating)
df <- iris[,1:4]
df$virgi <- as.numeric(iris$Species=="virginica")
svmMod <- svm(virgi ~ ., data = df)
svmPred <- predict(svmMod, df[,-5])
svmPredictiction <- prediction(svmPred, df$virgi)
setwd("/Users/naga/Downloads/My files/GitHub/Analytics-Projects")
